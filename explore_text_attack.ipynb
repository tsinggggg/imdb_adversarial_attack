{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 1: provide data and use existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading pre-trained model from HuggingFace model repository: \u001b[94mtextattack/bert-base-uncased-rotten-tomatoes\u001b[0m\n",
      "Using /tmp/tfhub_modules to cache modules.\n",
      "\u001b[34;1mtextattack\u001b[0m: Goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'> compatible with model BertForSequenceClassification.\n",
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  unk\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding_type):  paragramcf\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding_type):  paragramcf\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.904458599\n",
      "        (compare_with_original):  False\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n",
      "\u001b[34;1mtextattack\u001b[0m: Load time: 13.200591325759888s\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading model and tokenizer from file: None\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading module from `\u001b[94mtest_data.py\u001b[0m`.\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]--------------------------------------------- Result 1 ---------------------------------------------\n",
      "\u001b[91m0 (87%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "Carlyle Looks Toward Commercial Aerospace ( Reuters ) . Reuters - Private investment firm Carlyle Group , which has a reputation for making well - timed and occasionally controversial plays in the defense industry , has quietly placed its bets on another part of the market .\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Total] 0 / 0 / 1:  20%|█    | 1/5 [00:00<00:00,  6.15it/s]/home/qc/anaconda3/lib/python3.7/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings[len(transformed_texts) :]\n",
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "\u001b[91m0 (99%)\u001b[0m --> \u001b[92m1 (71%)\u001b[0m\n",
      "\n",
      "Oil and Economy \u001b[91mCloud\u001b[0m Stocks ' Outlook ( Reuters ) . Reuters - \u001b[91mSoaring\u001b[0m crude prices plus \u001b[91mworries\u001b[0m about the economy and the \u001b[91moutlook\u001b[0m for earnings are expected to \u001b[91mhang\u001b[0m over the stock market next \u001b[91mweek\u001b[0m during the depth of the \u001b[91msummer\u001b[0m doldrums .\n",
      "\n",
      "Oil and Economy \u001b[92mClouds\u001b[0m Stocks ' Outlook ( Reuters ) . Reuters - \u001b[92mAscending\u001b[0m crude prices plus \u001b[92mconcern\u001b[0m about the economy and the \u001b[92mprospects\u001b[0m for earnings are expected to \u001b[92mheng\u001b[0m over the stock market next \u001b[92mweeks\u001b[0m during the depth of the \u001b[92msummertime\u001b[0m doldrums .\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Total] 1 / 0 / 2:  40%|██   | 2/5 [00:02<00:03,  1.09s/it]--------------------------------------------- Result 3 ---------------------------------------------\n",
      "\u001b[91m0 (85%)\u001b[0m --> \u001b[92m1 (51%)\u001b[0m\n",
      "\n",
      "Iraq \u001b[91mHalts\u001b[0m Oil \u001b[91mExports\u001b[0m from \u001b[91mMain\u001b[0m \u001b[91mSouthern\u001b[0m Pipeline ( Reuters ) . Reuters - Authorities have halted oil export flows from the \u001b[91mmain\u001b[0m pipeline in southern Iraq after intelligence showed a rebel militia could strike infrastructure , an oil official said on Saturday .\n",
      "\n",
      "Iraq \u001b[92mArrested\u001b[0m Oil \u001b[92mExits\u001b[0m from \u001b[92mImperative\u001b[0m \u001b[92mEasterly\u001b[0m Pipeline ( Reuters ) . Reuters - Authorities have halted oil export flows from the \u001b[92msignificant\u001b[0m pipeline in southern Iraq after intelligence showed a rebel militia could strike infrastructure , an oil official said on Saturday .\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Total] 2 / 0 / 3:  60%|███  | 3/5 [00:04<00:02,  1.43s/it]--------------------------------------------- Result 4 ---------------------------------------------\n",
      "\u001b[91m0 (74%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "Oil prices soar to all - time record , posing new menace to US economy ( AFP ) . AFP - Tearaway world oil prices , toppling records and straining wallets , present a new economic menace barely three months before the US presidential elections .\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Total] 2 / 0 / 4:  80%|████ | 4/5 [00:04<00:01,  1.08s/it]--------------------------------------------- Result 5 ---------------------------------------------\n",
      "\u001b[91m0 (77%)\u001b[0m --> \u001b[92m1 (53%)\u001b[0m\n",
      "\n",
      "Stocks End Up , But Near Year \u001b[91mLows\u001b[0m ( Reuters ) . Reuters - Stocks ended slightly higher on Friday but stayed near lows for the year as oil prices surged past # 36;46 a barrel , offsetting a positive outlook from computer maker Dell Inc. ( DELL.O )\n",
      "\n",
      "Stocks End Up , But Near Year \u001b[92mPlunges\u001b[0m ( Reuters ) . Reuters - Stocks ended slightly higher on Friday but stayed near lows for the year as oil prices surged past # 36;46 a barrel , offsetting a positive outlook from computer maker Dell Inc. ( DELL.O )\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Total] 3 / 0 / 5: 100%|█████| 5/5 [00:05<00:00,  1.04s/it]\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 3      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 2      |\n",
      "| Original accuracy:            | 60.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 11.32% |\n",
      "| Average num. words per input: | 38.8   |\n",
      "| Avg num queries:              | 135.0  |\n",
      "+-------------------------------+--------+\n",
      "\u001b[34;1mtextattack\u001b[0m: Attack time: 5.226928234100342s\n"
     ]
    }
   ],
   "source": [
    "!textattack attack --recipe textfooler --dataset-from-file \"test_data.py\" --model bert-base-uncased-mr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example2(not working yet):provide model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from slack channel:\n",
    "\n",
    "Anyways, the easiest way to do this is:\n",
    "Implement a tokenizer with an encode method that turns a string into whatever input your model needs\n",
    "Create a file that loads your tokenizer and model, like myfile.py:\n",
    "# import whatever you need\n",
    "import pytorch_lightning\n",
    "# load the model\n",
    "model = MyModel()\n",
    "# Load the tokenizer, which must have `encode()` method\n",
    "tokenizer = MyTokenizer()\n",
    "Then you should be able to attack this model from the command-line:\n",
    "textattack attack --model-from-file myfile.py --recipe textfooler --num-examples 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading model and tokenizer from file: \u001b[94mmodel1.py\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading module from `\u001b[94mmodel1.py\u001b[0m`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/attack_args_helpers.py\", line 252, in parse_model_from_args\n",
      "    model_module = load_module_from_file(args.model_from_file)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/attack_args_helpers.py\", line 108, in load_module_from_file\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"model1.py\", line 23, in <module>\n",
      "    tokenizer = Tokenizer()\n",
      "  File \"model1.py\", line 15, in __init__\n",
      "    self.t = pd.read_pickle(\"./tf_idf_vectorizor.pkl\")\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\", line 186, in read_pickle\n",
      "    return pc.load(f, encoding=None)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/pandas/compat/pickle_compat.py\", line 241, in load\n",
      "    return up.load()\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/pickle.py\", line 1088, in load\n",
      "    dispatch[key[0]](self)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/pickle.py\", line 1385, in load_stack_global\n",
      "    self.append(self.find_class(module, name))\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/pandas/compat/pickle_compat.py\", line 180, in find_class\n",
      "    return super().find_class(module, name)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/pickle.py\", line 1428, in find_class\n",
      "    return _getattribute(sys.modules[module], name)[0]\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/pickle.py\", line 299, in _getattribute\n",
      "    .format(name, obj)) from None\n",
      "AttributeError: Can't get attribute 'spacy_tokenizer' on <module '__main__' from '/home/qc/anaconda3/bin/textattack'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qc/anaconda3/bin/textattack\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/textattack_cli.py\", line 41, in main\n",
      "    args.func.run(args)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/attack_command.py\", line 32, in run\n",
      "    run_single_threaded(args)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/run_attack_single_threaded.py\", line 47, in run\n",
      "    attack = parse_attack_from_args(args)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/attack_args_helpers.py\", line 185, in parse_attack_from_args\n",
      "    model = parse_model_from_args(args)\n",
      "  File \"/home/qc/anaconda3/lib/python3.7/site-packages/textattack/commands/attack/attack_args_helpers.py\", line 254, in parse_model_from_args\n",
      "    raise ValueError(f\"Failed to import file {args.model_from_file}\")\n",
      "ValueError: Failed to import file model1.py\n"
     ]
    }
   ],
   "source": [
    "!textattack attack --model-from-file model1.py --recipe deepwordbug --num-examples 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 3: (from doc)NLTK and Named Entity Recognition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "does not work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source : https://github.com/QData/TextAttack/blob/master/docs/examples/2_Constraints.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qc/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/qc/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/qc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # The NLTK tokenizer\n",
    "nltk.download('maxent_ne_chunker') # NLTK named-entity chunker\n",
    "nltk.download('words') # NLTK list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  2017/CD\n",
      "  ,/,\n",
      "  star/NN\n",
      "  quarterback/NN\n",
      "  (PERSON Tom/NNP Brady/NNP)\n",
      "  led/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION Patriots/NNP)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (ORGANIZATION Super/NNP Bowl/NNP)\n",
      "  ,/,\n",
      "  but/CC\n",
      "  lost/VBD\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (ORGANIZATION Philadelphia/NNP Eagles/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = ('In 2017, star quarterback Tom Brady led the Patriots to the Super Bowl, '\n",
    "           'but lost to the Philadelphia Eagles.')\n",
    "\n",
    "# 1. Tokenize using the NLTK tokenizer.\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 2. Tag parts of speech using the NLTK part-of-speech tagger.\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# 3. Extract entities from tagged sentence.\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Tom', 'NNP'), ('Brady', 'NNP')]), Tree('ORGANIZATION', [('Patriots', 'NNP')]), Tree('ORGANIZATION', [('Super', 'NNP'), ('Bowl', 'NNP')]), Tree('ORGANIZATION', [('Philadelphia', 'NNP'), ('Eagles', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "# 4. Filter entities to just named entities.\n",
    "named_entities = [entity for entity in entities if isinstance(entity, nltk.tree.Tree)]\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=2**14)\n",
    "def get_entities(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    # Setting `binary=True` makes NLTK return all of the named\n",
    "    # entities tagged as NNP instead of detailed tags like\n",
    "    #'Organization', 'Geo-Political Entity', etc.\n",
    "    entities = nltk.chunk.ne_chunk(tagged, binary=True)\n",
    "    return entities.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 'NNP'),\n",
       " ('Black', 'NNP'),\n",
       " ('starred', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('2003', 'CD'),\n",
       " ('film', 'NN'),\n",
       " ('classic', 'JJ'),\n",
       " ('``', '``'),\n",
       " ('School', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Rock', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Jack Black starred in the 2003 film classic \"School of Rock\".'\n",
    "get_entities(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "from textattack.constraints import Constraint\n",
    "\n",
    "class NamedEntityConstraint(Constraint):\n",
    "    \"\"\" A constraint that ensures `transformed_text` only substitutes named entities from `current_text` with other named entities.\n",
    "    \"\"\"\n",
    "    def _check_constraint(self, tranformed_text, current_text, original_text=None):\n",
    "        transformed_entities = get_entities(transformed_text.text)\n",
    "        current_entities = get_entities(current_text.text)\n",
    "        # If there aren't named entities, let's return False (the attack\n",
    "        # will eventually fail).\n",
    "        if len(current_entities) == 0:\n",
    "            return False\n",
    "        if len(current_entities) != len(transformed_entities):\n",
    "            # If the two sentences have a different number of entities, then \n",
    "            # they definitely don't have the same labels. In this case, the \n",
    "            # constraint is violated, and we return False.\n",
    "            return False\n",
    "        else:\n",
    "            # Here we compare all of the words, in order, to make sure that they match.\n",
    "            # If we find two words that don't match, this means a word was swapped \n",
    "            # between `current_text` and `transformed_text`. That word must be a named entity to fulfill our\n",
    "            # constraint.\n",
    "            current_word_label = None\n",
    "            transformed_word_label = None\n",
    "            for (word_1, label_1), (word_2, label_2) in zip(current_entities, transformed_entities):\n",
    "                if word_1 != word_2:\n",
    "                    # Finally, make sure that words swapped between `x` and `x_adv` are named entities. If \n",
    "                    # they're not, then we also return False.\n",
    "                    if (label_1 not in ['NNP', 'NE']) or (label_2 not in ['NNP', 'NE']):\n",
    "                        return False            \n",
    "            # If we get here, all of the labels match up. Return True!\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textattack.datasets.classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0c4e7f0bd64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYelpSentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTMForYelpSentimentClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMForYelpSentimentClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textattack.datasets.classification'"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "from textattack.datasets.classification import YelpSentiment\n",
    "# Create the model.\n",
    "from textattack.models.classification.lstm import LSTMForYelpSentimentClassification\n",
    "model = LSTMForYelpSentimentClassification()\n",
    "# Create the goal function using the model.\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "goal_function = UntargetedClassification(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cannot find the imported items in github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 4: follow doc example to attack a sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logistic_reg = pd.read_pickle('./tf_idf_logistic_reg.pkl')\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    mytokens = nlp(sentence)\n",
    "    mytokens = [word.lemma_ for word in mytokens if not word.is_punct and not word.is_stop]\n",
    "    return mytokens\n",
    "\n",
    "t = pd.read_pickle('./tf_idf_vectorizor.pkl')\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, t, m):\n",
    "        self.tokenizer = t\n",
    "        self.tokenizer.encode = self.tokenizer.transform\n",
    "        self.model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class '__main__.Model'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    }
   ],
   "source": [
    "from textattack.goal_functions import UntargetedClassification\n",
    "model = Model(t, logistic_reg)\n",
    "goal_function = UntargetedClassification(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  15\n",
      "    (embedding_type):  paragramcf\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "    (1): StopwordModification\n",
      "  (is_black_box):  True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.search_methods import GreedySearch\n",
    "from textattack.shared import Attack\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "\n",
    "# We're going to the `WordSwapEmbedding` transformation. Using the default settings, this\n",
    "# will try substituting words with their neighbors in the counter-fitted embedding space. \n",
    "transformation = WordSwapEmbedding(max_candidates=15) \n",
    "\n",
    "# We'll use the greedy search method again\n",
    "search_method = GreedySearch()\n",
    "\n",
    "# Our constraints will be the same as Tutorial 1, plus the named entity constraint\n",
    "constraints = [RepeatModification(),\n",
    "               StopwordModification(),\n",
    "               #NamedEntityConstraint()\n",
    "              ]\n",
    "\n",
    "# Now, let's make the attack using these parameters. \n",
    "attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "print(attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc.extra.datasets\n",
    "train, test = thinc.extra.datasets.imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Failed to predict with model <class '__main__.Model'>. Check tokenizer configuration.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3813bee7456f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_successes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mnum_successes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSuccessfulAttackResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_attack_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/attack.py\u001b[0m in \u001b[0;36mattack_dataset\u001b[0;34m(self, dataset, indices)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_examples_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mgoal_function_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgoal_function_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_status\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGoalFunctionResultStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKIPPED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mSkippedAttackResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal_function_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/attack.py\u001b[0m in \u001b[0;36m_get_examples_from_dataset\u001b[0;34m(self, dataset, indices)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 )\n\u001b[1;32m    224\u001b[0m                 goal_function_result, _ = self.goal_function.init_attack_example(\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mattacked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 )\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mgoal_function_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36minit_attack_example\u001b[0;34m(self, attacked_text, ground_truth_output)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_truth_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, attacked_text, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mAttackedText\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattacked_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_over\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(self, attacked_text_list, check_skip)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mattacked_text_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacked_text_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mqueries_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattacked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked_text_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdisplayed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_displayed_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36m_call_model\u001b[0;34m(self, attacked_text_list)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             ]\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncached_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncached_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/goal_functions/goal_function.py\u001b[0m in \u001b[0;36m_call_model_uncached\u001b[0;34m(self, attacked_text_list)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             outputs = batch_model_predict(\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             )\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/utils/tensor.py\u001b[0m in \u001b[0;36mbatch_model_predict\u001b[0;34m(model, inputs, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/utils/tensor.py\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;34mf\"Failed to predict with model {model.__class__}. Check tokenizer configuration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/utils/tensor.py\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_model_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         textattack.shared.utils.logger.error(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/utils/tensor.py\u001b[0m in \u001b[0;36mtry_model_predict\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_model_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textattack/shared/utils/tensor.py\u001b[0m in \u001b[0;36mget_model_device\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmodel_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "\n",
    "results_iterable = attack.attack_dataset(train[:20]) #, attack_n=True) this argument does not exist\n",
    "logger = CSVLogger(color_method='html')\n",
    "\n",
    "num_successes = 0\n",
    "while num_successes < 10:\n",
    "    result = next(results_iterable)\n",
    "    if isinstance(result, SuccessfulAttackResult):\n",
    "        logger.log_attack_result(result)\n",
    "        num_successes += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "looks like pytorch syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 5: follow doc example to attack a pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\n",
    "              is_bidirectional=False, dropout=0.0, output_dim=1, padding_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
    "                                      padding_idx=padding_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                            num_layers=n_layers, bidirectional=is_bidirectional,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear((is_bidirectional+1)*hidden_dim, output_dim)\n",
    "\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "    \n",
    "    def forward(self, input_sequence, sequence_length):\n",
    "    \n",
    "        embeddings = self.embedding(input_sequence)\n",
    "\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, \n",
    "                                                            sequence_length)\n",
    "\n",
    "        packed_output, (hidden_state, cell_state) = self.lstm(packed_embeddings)\n",
    "\n",
    "        if self.is_bidirectional:\n",
    "            output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        else:\n",
    "            output = hidden_state[-1,:,:]\n",
    "        scores = self.fc(output)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language='en_core_web_md', lower=True, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors='glove.6B.300d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300 # This needs to match the size of the pre-trained embeddings!\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = ImdbRNN(vocab_size=vocab_size, embedding_dim=embedding_dim,hidden_dim=hidden_dim,\n",
    "                  n_layers=num_layers,  dropout=dropout,\n",
    "                  padding_idx=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImdbRNN(\n",
       "  (embedding): Embedding(25002, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 256, num_layers=3, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.models.helpers.lstm_for_classification import LSTMForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LSTMForClassification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
