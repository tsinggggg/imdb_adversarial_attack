{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import thinc.extra.datasets\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84123648/84125825 [============================>.] - ETA: 0s Untaring file...\n"
     ]
    }
   ],
   "source": [
    "train, test = thinc.extra.datasets.imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.0/en_core_web_md-2.3.0.tar.gz (50.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.8 MB 937 kB/s eta 0:00:01     |██████████████████████▎         | 35.3 MB 1.2 MB/s eta 0:00:14     |██████████████████████▌         | 35.6 MB 1.2 MB/s eta 0:00:13\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from en_core_web_md==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (4.44.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/qc/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/qc/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/qc/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/qc/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/qc/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/qc/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/qc/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.2.0)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.0-py3-none-any.whl size=50921514 sha256=f91a1193e7be0cd42ddbec262aad1c752456f317e0e2aca31bd30fc3da0588d8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mtl8x5wt/wheels/a9/30/7d/40a0d13f1ddae5b6398c9f407391942152348eb9eae62fa21e\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    mytokens = nlp(sentence)\n",
    "    mytokens = [ word.lemma_ for word in mytokens if not word.is_punct and not word.is_stop]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train, columns=['x', 'y'])\n",
    "df_test = pd.DataFrame(test, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('tfidf', TfidfVectorizer(tokenizer=spacy_tokenizer)),\n",
    "                           ('clf', LogisticRegression())\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x7fd855b045f0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20-30 minutes\n",
    "pipeline.fit(df_train['x'], df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(pipeline, './tf_idf_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 mins\n",
    "y_pred_test = pipeline.predict_proba(df_test['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9485079168000001"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(df_test['y'], y_pred_test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['y'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language='en_core_web_md', lower=True, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:26<00:00, 3.22MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:05, 2.03MB/s]                               \n",
      "100%|█████████▉| 399226/400000 [00:34<00:00, 10642.58it/s]"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors='glove.6B.300d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\n",
    "              is_bidirectional=False, dropout=0.0, output_dim=1, padding_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
    "                                      padding_idx=padding_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                            num_layers=n_layers, bidirectional=is_bidirectional,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear((is_bidirectional+1)*hidden_dim, output_dim)\n",
    "\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "    \n",
    "    def forward(self, input_sequence, sequence_length):\n",
    "    \n",
    "        embeddings = self.embedding(input_sequence)\n",
    "\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, \n",
    "                                                            sequence_length)\n",
    "\n",
    "        packed_output, (hidden_state, cell_state) = self.lstm(packed_embeddings)\n",
    "\n",
    "        if self.is_bidirectional:\n",
    "            output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        else:\n",
    "            output = hidden_state[-1,:,:]\n",
    "        scores = self.fc(output)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300 # This needs to match the size of the pre-trained embeddings!\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = ImdbRNN(vocab_size=vocab_size, embedding_dim=embedding_dim,hidden_dim=hidden_dim,\n",
    "                  n_layers=num_layers,  dropout=dropout,\n",
    "                  padding_idx=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word embeddings\n",
    "glove_vectors = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(glove_vectors)\n",
    "# Zero out <unk> and <pad> tokens\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "# Define our loss function, optimizer, and move things to GPU\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "def roc(scores, y):    \n",
    "    scores = torch.sigmoid(scores).detach().cpu()\n",
    "    return sklearn.metrics.roc_auc_score(y.cpu(), scores)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = roc(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = roc(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "      \n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.6623532661121257 val_loss : 0.6884514497498334\n",
      "train_accuracy : 63.10034960354696 val_accuracy : 66.15630441180073\n",
      "Validation loss decreased (inf --> 0.688451).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.6889677321823844 val_loss : 0.69243919142222\n",
      "train_accuracy : 55.66974941201753 val_accuracy : 75.55368059955272\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.5672216991873553 val_loss : 0.45616439136408143\n",
      "train_accuracy : 81.30125140256432 val_accuracy : 90.91957832496959\n",
      "Validation loss decreased (0.688451 --> 0.456164).  Saving model ...\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.30173119065100257 val_loss : 0.29135100189912116\n",
      "train_accuracy : 95.51699222123537 val_accuracy : 95.15840968623243\n",
      "Validation loss decreased (0.456164 --> 0.291351).  Saving model ...\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.15816742665793773 val_loss : 0.32047024268214985\n",
      "train_accuracy : 98.42993224657101 val_accuracy : 95.70143335059566\n",
      "==================================================\n",
      "Test Loss: 0.376\n",
      "Test Acc: 94.47%\n"
     ]
    }
   ],
   "source": [
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/\")\n",
    "num_epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(num_epochs):    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    # Log the training results\n",
    "    summary_writer.add_scalar(\"training/accuracy\", train_acc, epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    \n",
    "    # Log the validation results\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", valid_acc, epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", valid_loss, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {train_loss} val_loss : {valid_loss}')\n",
    "    print(f'train_accuracy : {train_acc*100} val_accuracy : {valid_acc*100}')\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_valid_loss,\n",
    "                                                                                        valid_loss))\n",
    "        best_valid_loss = min(best_valid_loss, valid_loss)\n",
    "    print(25*'==')\n",
    " \n",
    "   \n",
    "# After completing all epochs, visualize our word vectors\n",
    "vecs = model.embedding.weight.data\n",
    "labels = [l.encode('utf8') for l in TEXT.vocab.itos]\n",
    "summary_writer.add_embedding(vecs, \n",
    "                             metadata=labels)\n",
    "summary_writer.close()\n",
    "# Print test performance\n",
    "test_loss, test_accuracy = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}\\nTest Acc: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use wrapper from textattact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', \n",
    "                  tokenizer_language='en_core_web_md', \n",
    "                  lower=True, \n",
    "                  include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors='glove.6B.300d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.models.helpers import LSTMForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc.extra.datasets\n",
    "train, test = thinc.extra.datasets.imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.commands.train_model.run_training import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.output_dir = './ta_model/'\n",
    "        self.enable_wandb = False\n",
    "        self.allowed_labels = []\n",
    "        self.batch_size = 128 \n",
    "        self.grad_accum_steps = 1\n",
    "        self.num_train_epochs = 5\n",
    "        self.learning_rate = 0.001\n",
    "        self.warmup_proportion = 0.1\n",
    "        self.max_length = 256\n",
    "        self.weights_name = 'pytorch_model.bin'\n",
    "        self.config_name = 'config.json'\n",
    "        self.tb_writer_step = 1000\n",
    "        self.checkpoint_steps = -1\n",
    "        self.checkpoint_every_epoch = True\n",
    "        self.early_stopping_epochs = -1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMForClassification(hidden_size=256, depth=3, dropout=0.5, max_seq_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300 # This needs to match the size of the pre-trained embeddings!\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = ImdbRNN(vocab_size=vocab_size, embedding_dim=embedding_dim,hidden_dim=hidden_dim,\n",
    "                  n_layers=num_layers,  dropout=dropout,\n",
    "                  padding_idx=pad_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
